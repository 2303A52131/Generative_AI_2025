{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD0kgG_KfVzT",
        "outputId": "3ddf72f9-447c-4e29-a078-b37beaf3faf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics calculated from scratch:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Metrics calculated using libraries:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Linear Regression Model:\n",
            "Slope (Coefficient): 1.0070000000000001\n",
            "Intercept: 0.17999999999999972\n",
            "Predicted values using Linear Regression: [20.32 30.39 40.46 50.53 60.6 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import math\n",
        "\n",
        "Y_actual = np.array([20, 30, 40, 50, 60])\n",
        "Y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "n = len(Y_actual)\n",
        "\n",
        "mae_scratch = sum(abs(Y_actual - Y_pred)) / n\n",
        "\n",
        "mse_scratch = sum((Y_actual - Y_pred) ** 2) / n\n",
        "\n",
        "rmse_scratch = math.sqrt(mse_scratch)\n",
        "\n",
        "print(\"Metrics calculated from scratch:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_scratch}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_scratch}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_scratch}\")\n",
        "\n",
        "mae_library = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_library = mean_squared_error(Y_actual, Y_pred)\n",
        "rmse_library = math.sqrt(mse_library)\n",
        "\n",
        "print(\"\\nMetrics calculated using libraries:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_library}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_library}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_library}\")\n",
        "\n",
        "X = np.array(Y_actual).reshape(-1, 1)\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y_pred)\n",
        "\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "print(\"\\nLinear Regression Model:\")\n",
        "print(f\"Slope (Coefficient): {slope}\")\n",
        "print(f\"Intercept: {intercept}\")\n",
        "\n",
        "Y_pred_lr = model.predict(X)\n",
        "print(f\"Predicted values using Linear Regression: {Y_pred_lr}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "Y_actual = np.array([0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2])\n",
        "Y_pred = np.array([0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2])\n",
        "\n",
        "n = len(Y_actual)\n",
        "\n",
        "true_positive = {0: 0, 1: 0, 2: 0}\n",
        "false_positive = {0: 0, 1: 0, 2: 0}\n",
        "false_negative = {0: 0, 1: 0, 2: 0}\n",
        "true_negative = {0: 0, 1: 0, 2: 0}\n",
        "\n",
        "for i in range(n):\n",
        "    for cls in [0, 1, 2]:\n",
        "        if Y_actual[i] == cls and Y_pred[i] == cls:\n",
        "            true_positive[cls] += 1\n",
        "        elif Y_actual[i] != cls and Y_pred[i] == cls:\n",
        "            false_positive[cls] += 1\n",
        "        elif Y_actual[i] == cls and Y_pred[i] != cls:\n",
        "            false_negative[cls] += 1\n",
        "        elif Y_actual[i] != cls and Y_pred[i] != cls:\n",
        "            true_negative[cls] += 1\n",
        "\n",
        "accuracy_scratch = sum([true_positive[cls] for cls in [0, 1, 2]]) / n\n",
        "\n",
        "precision_scratch = {cls: true_positive[cls] / (true_positive[cls] + false_positive[cls]) if (true_positive[cls] + false_positive[cls]) > 0 else 0 for cls in [0, 1, 2]}\n",
        "recall_scratch = {cls: true_positive[cls] / (true_positive[cls] + false_negative[cls]) if (true_positive[cls] + false_negative[cls]) > 0 else 0 for cls in [0, 1, 2]}\n",
        "f1_score_scratch = {cls: 2 * precision_scratch[cls] * recall_scratch[cls] / (precision_scratch[cls] + recall_scratch[cls]) if (precision_scratch[cls] + recall_scratch[cls]) > 0 else 0 for cls in [0, 1, 2]}\n",
        "\n",
        "print(\"Metrics calculated from scratch:\")\n",
        "print(f\"Accuracy: {accuracy_scratch}\")\n",
        "for cls in [0, 1, 2]:\n",
        "    print(f\"Class {cls} - Precision: {precision_scratch[cls]}, Recall: {recall_scratch[cls]}, F1-Score: {f1_score_scratch[cls]}\")\n",
        "\n",
        "accuracy_library = accuracy_score(Y_actual, Y_pred)\n",
        "precision_library = precision_score(Y_actual, Y_pred, average=None)\n",
        "recall_library = recall_score(Y_actual, Y_pred, average=None)\n",
        "f1_library = f1_score(Y_actual, Y_pred, average=None)\n",
        "\n",
        "print(\"\\nMetrics calculated using libraries:\")\n",
        "print(f\"Accuracy: {accuracy_library}\")\n",
        "for cls, (p, r, f1) in enumerate(zip(precision_library, recall_library, f1_library)):\n",
        "    print(f\"Class {cls} - Precision: {p}, Recall: {r}, F1-Score: {f1}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(Y_actual, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JbdeOYSjCA8",
        "outputId": "01f63ec2-97c2-4751-fd93-7363dded4f03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics calculated from scratch:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7000000000000001\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n",
            "\n",
            "Metrics calculated using libraries:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.64      0.70        11\n",
            "           1       0.71      0.62      0.67         8\n",
            "           2       0.71      0.91      0.80        11\n",
            "\n",
            "    accuracy                           0.73        30\n",
            "   macro avg       0.74      0.72      0.72        30\n",
            "weighted avg       0.74      0.73      0.73        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}